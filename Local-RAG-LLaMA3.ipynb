{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33cb666e",
   "metadata": {},
   "source": [
    "# Local RAG using LLaMA3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a538d5",
   "metadata": {},
   "source": [
    "We show a really simple example of how to use the Retrieval-Augmented Generation (RAG) model with a Support Vector Machine (SVM) classifier to perform a text classification task (not using a vector database). The RAG model is a powerful language model that can retrieve relevant information from a knowledge base and use it to generate high-quality text.\n",
    "\n",
    "This notebook is based on [1](https://github.com/githubpradeep/notebooks/blob/main/Llama3%20RAG%20SVM.ipynb) and original Langchain notebook [2](https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb), of which, however, it represents a notable simplification in that it does not use a vector database (instead, it uses a simple SVM classifier to perform the classification task).\n",
    "\n",
    "To run this notebook, a Python + Jupyter environment is needed together with a [Groq Cloud Playground](https://console.groq.com/playground) API key (you have to sign in using email or other accounts from other services)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56789c0",
   "metadata": {},
   "source": [
    "All the modules needed are as follows (uncomment to install)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "911c102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain_openai langchain_groq gpt4all scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d328722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Index\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.retrievers import SVMRetriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.retrievers import SVMRetriever\n",
    "\n",
    "# Add the additional URL to the list\n",
    "urls = [\n",
    "    \"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\",\n",
    "    \"https://research.ibm.com/blog/retrieval-augmented-generation-RAG\",\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load the web pages into a list of documents\n",
    "docs = [WebBaseLoader(url).load() for url in urls] \n",
    "docs_list = [item for sublist in docs for item in sublist] # \n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, chunk_overlap=0\n",
    ") \n",
    "doc_splits = text_splitter.split_documents(docs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf59b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "# Prompt template for the question-answering task\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Initializes a ChatGroq language model with the specified temperature, model name, and API key.\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\", groq_api_key='***INSERT API KEY HERE***')\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    \n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser() # \n",
    "\n",
    "# Run\n",
    "question = \"agent memory\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "328f71bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT4All embeddings model\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings \n",
    "embeddings = GPT4AllEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a685c29d",
   "metadata": {},
   "source": [
    "The purpose of the following code is to create a class called `Retriever` that can retrieve the most relevant documents from a collection of documents based on a given query (question). It takes two inputs: a list of documents (`docs`) and a set of pre-computed embeddings (`embeddings`) for those documents.\n",
    "\n",
    "The class initializes by creating a numpy array (`embeds_np`) containing the embeddings for each document, normalized using L2 normalization. This normalization ensures that the embeddings have a consistent scale, which is important for the subsequent similarity calculations.\n",
    "\n",
    "The main functionality of the class is provided by the `query` method. This method takes a question (query) as input and an optional parameter `k`, which specifies the number of top relevant documents to return (default is 3).\n",
    "\n",
    "The query method first embeds the input `question` and normalizes it using L2 normalization. It then concatenates the normalized question embedding with the normalized document embeddings (`embeds_np`) into a single array (`x`). It also creates a label array (`y`) with the first element - corresponding to the query/question vector, indicating that it's the target label for the SVM training - labeled as positive (1) and the rest labeled as negative (0). \n",
    "\n",
    "Next, the code trains a Support Vector Machine (SVM) classifier using the concatenated array `x` and the label array `y`. The SVM is a machine learning algorithm that can learn to separate the positive and negative examples in the data. In this case, it learns to separate the question embedding from the document embeddings, effectively learning the similarity between the question and each document.\n",
    "\n",
    "After training the SVM, the code computes the similarity scores between the question and each document using the `decision_function` method of the SVM. These similarity scores are then sorted in descending order, and the indices of the top k documents are retrieved.\n",
    "\n",
    "Finally, the method returns a list of the top `k` most relevant documents based on the sorted indices.\n",
    "\n",
    "The key data transformation happening here is the normalization of the embeddings and the concatenation of the question embedding with the document embeddings. This allows the SVM to learn the similarity between the question and each document in a consistent and efficient manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32689369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "\n",
    "class Retriever():\n",
    "    def __init__(self, docs, embeddings): \n",
    "        self.embeddings = embeddings\n",
    "        self.docs = docs\n",
    "        x = [doc_split.page_content for doc_split in docs]\n",
    "        embeds = embeddings.embed_documents(x)\n",
    "        embeds_np = np.array(embeds)\n",
    "        embeds_np = embeds_np / np.sqrt((embeds_np**2).sum(1, keepdims=True)) # L2 normalize the rows\n",
    "        self.embeds = embeds_np\n",
    "\n",
    "    # This method is responsible for retrieving the top k most relevant documents \n",
    "    # from a collection of documents based on a given query (question).     \n",
    "    def query(self, question, k=3): # k is the number of top results to return\n",
    "        query = np.array(self.embeddings.embed_query(question))\n",
    "\n",
    "        query = query / np.sqrt((query**2).sum()) # L2 normalize the query\n",
    "        x = np.concatenate([[query], self.embeds])\n",
    "        y = np.zeros(len(x))  # initialize labels\n",
    "        y[0] = 1 # set the first element (the query) as positive\n",
    "\n",
    "        # SVM training \n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "        clf = svm.LinearSVC(class_weight='balanced', verbose=False, max_iter=50000, tol=1e-5, C=1)\n",
    "        clf.fit(x, y) # train\n",
    "\n",
    "        similarities = clf.decision_function(x) # compute the similarity scores\n",
    "        sorted_ix = np.argsort(-similarities)[1:] # sort in descending order and skip the first element (the query)\n",
    "        res = []\n",
    "        for i in sorted_ix[:k]:\n",
    "            res.append(self.docs[i-1])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac6d6746",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(doc_splits, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a276c529",
   "metadata": {},
   "source": [
    "The purpose of this code is to retrieve relevant information from a knowledge base and generate a response to a given question using a RAG model.\n",
    "\n",
    "The input to this code is a question string `q`.\n",
    "\n",
    "The output of this code is a generated response that lists the \"types of adversarial attacks\" based on the retrieved information and the given question.\n",
    "\n",
    "Here's how the code achieves its purpose. The question string `q` is assigned some value '...'.\n",
    "The `retriever.query(q)` function is called, which retrieves relevant information on the given question `q`. The retrieved information is stored in the variable `d`.\n",
    "The `rag_chain.invoke` function is called, which takes the retrieved information `d` and the original question `q` as input. This function likely the RAG model to generate a response based on the retrieved context and the question.\n",
    "The generated response is stored in the variable `generation` and finally printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'List the types of Adversarial Attacks'\n",
    "d = retriever.query(q)\n",
    "generation = rag_chain.invoke({\"context\": d, \"question\": q})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55f6b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'What is agent memory?'\n",
    "d = retriever.query(q)\n",
    "generation = rag_chain.invoke({\"context\": d, \"question\": q})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad048089",
   "metadata": {},
   "outputs": [],
   "source": [
    "q='List all memory types'\n",
    "d = retriever.query(q)\n",
    "generation = rag_chain.invoke({\"context\": d, \"question\": q})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27575e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "q='Explain \"Tree of Thoughts\"'\n",
    "d = retriever.query(q)\n",
    "generation = rag_chain.invoke({\"context\": d, \"question\": q})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f140c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "q='What is RAG?'\n",
    "d = retriever.query(q)\n",
    "generation = rag_chain.invoke({\"context\": d, \"question\": q})\n",
    "print(generation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
